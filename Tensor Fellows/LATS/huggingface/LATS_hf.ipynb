{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install All Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "# !pip install llama-index-llms-huggingface\n",
    "# !pip install llama-index-embeddings-huggingface\n",
    "# !pip install llama-index-embeddings-huggingface-api\n",
    "# !pip install --upgrade pip\n",
    "# !pip install llama-index-agent-lats\n",
    "# !pip install clingo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"dataset/task_1_plan_generation.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "queries = [instance['query'] for instance in data['instances']]\n",
    "\n",
    "system_prompts = []\n",
    "initial_states = []\n",
    "goal_states = []\n",
    "answers = []\n",
    "for query in queries:\n",
    "    system = query.split(\"[STATEMENT]\")\n",
    "    system_prompts.append(system[0])\n",
    "    initial = system[1].split(\"My goal\")\n",
    "    initial_states.append(initial[0])\n",
    "    goal = initial[1].split(\"[PLAN]\")\n",
    "    goal_states.append(\"My goal\" + goal[0])\n",
    "    answers.append(goal[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Llama-3.1-8B-instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kelly\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kelly\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kelly\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kelly\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\kelly\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "def initialize_llama(system_prompt):\n",
    "    hf_token = \"\" # REPLACE WITH PERSONAL TOKEN\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        token=hf_token,\n",
    "    )\n",
    "\n",
    "    stopping_ids = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "    ]\n",
    "\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        system_prompt = system_prompt,\n",
    "        max_new_tokens=256,\n",
    "        model_kwargs={\n",
    "            \"token\": hf_token,\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "        },\n",
    "        generate_kwargs={\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_p\": 0.9,\n",
    "        },\n",
    "        tokenizer_name=tokenizer,\n",
    "        tokenizer_kwargs={\"token\": hf_token},\n",
    "        stopping_ids=stopping_ids,\n",
    "    )\n",
    "    \n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LATS Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def initialize_tool(llm):\n",
    "    prompt_template = f\"\"\"\n",
    "    I am working on a Blocks World problem, and here is a proposed next step to solve the prolem:\n",
    "\n",
    "    Proposed Solution:\n",
    "    {query}\n",
    "\n",
    "    Is this solution valid? Does it work towards the goal?\n",
    "\n",
    "    Please respond with \"valid\" if the solution works towards the goal or \"not valid\" if it doesn't.\n",
    "    \"\"\"\n",
    "\n",
    "    responses = []\n",
    "\n",
    "    # Ask the LLM 5 times\n",
    "    for _ in range(5):\n",
    "        response = llm.complete(prompt_template).strip().lower()\n",
    "        responses.append(response)\n",
    "\n",
    "    # Count the occurrences of \"valid\" and \"not valid\"\n",
    "    counts = Counter(responses)\n",
    "\n",
    "    # Determine majority response\n",
    "    if counts[\"valid\"] >= 3:\n",
    "        return \"The solution is good!\"\n",
    "    else:\n",
    "        return \"The solution is not good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "class LATS:\n",
    "    def __init__(self, system):\n",
    "        llm = initialize_llama(system)\n",
    "        tool = initialize_tool(llm)\n",
    "\n",
    "        query_engine_tools = [\n",
    "            QueryEngineTool(\n",
    "                query_engine=tool,\n",
    "                metadata=ToolMetadata(\n",
    "                    name=\"blocks_world_tool\",\n",
    "                    description=(\n",
    "                        '''\n",
    "                        Provide a proposed solution for the next step to the group of LLMs\n",
    "                        along with the current state of the problem space.\n",
    "                        It will provide a majority ruling as to whether or not it is a valid\n",
    "                        solution.\n",
    "                        '''\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        agent_worker = LATSAgentWorker(\n",
    "            tools=query_engine_tools,\n",
    "            llm=llm,\n",
    "            num_expansions=2,\n",
    "            max_rollouts=3,\n",
    "            verbose=True,\n",
    "        )\n",
    "        self.agent = agent_worker.as_agent()\n",
    "\n",
    "    def call(self, initial, goal):\n",
    "        response = self.agent.chat(initial + \"\\n[GOAL]\\n\\n\" + goal)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Agent with Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'system_prompts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m responses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(system_prompts)):\n\u001b[0;32m      3\u001b[0m     system_statement \u001b[38;5;241m=\u001b[39m system_prompts[i]\n\u001b[0;32m      4\u001b[0m     initial_statement \u001b[38;5;241m=\u001b[39m initial_states[i]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'system_prompts' is not defined"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for i in range(len(system_prompts)):\n",
    "    system_statement = system_prompts[i]\n",
    "    initial_statement = initial_states[i]\n",
    "    goal_statement = goal_states[i]\n",
    "    lats = LATS(system_statement)\n",
    "    responses.append(lats.call(initial_statement, goal_statement))\n",
    "\n",
    "# lats = LATS(system_prompts[0])\n",
    "# response = lats.call(initial_states[0], goal_states[0])\n",
    "# response\n",
    "print(\"response: \" + responses[0], \"\\nanswer: \", answers[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
